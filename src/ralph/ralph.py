import sys
import subprocess
from dataclasses import dataclass
import dotenv
import os
import asyncio
import json

from openai import OpenAI
from loguru import logger
from mcp.client import MCPClient

dotenv.load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
logger.remove()
logger.add(sys.stderr, level="INFO", format="{time} {level} {message}")


def llm_completion(messages, model="gpt-4o-mini", temperature=0.0, functions=[]):
    """Completes a prompt using the LLM, specifying available tools."""
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=temperature,
        functions=functions,
    )
    return response.choices[0].message


class Ralph:
    def __init__(self, agent: str, task: str, server_stdin=None, server_stdout=None):
        self.agent = agent
        self.task = task
        self.messages = []
        self.tasks = []
        self.mcp_client = MCPClient(reader=server_stdout, writer=server_stdin)
    
    async def _query_tools(self):
        tools_response = await self.mcp_client.list_tools()
        tools = tools_response.get("result", {}).get("tools", [])
        self.tools = [
            {
                "name": tool["name"],
                "description": tool["description"],
                "parameters": tool["inputSchema"]
            } for tool in tools
        ]

    async def execute(self):
        """Executes Ralph's task in a loop until completion."""
        await self._query_tools()

        if not self.tasks:
            # no tasks, start to plan
            task = "Create a list of independent tasks to complete the following: " + self.task
            task += "\n\nReply with a plaintext JSON array of tasks (not markdown), each task should be a string."
            messages = [
                {"role": "system", "content": self.agent},
                {"role": "user", "content": task}
            ]
            response = llm_completion(messages, functions=self.tools)

            logger.info(response.model_dump())

            # TODO: harden
            self.tasks = json.loads(response.content) if response.content else []

        # call ralph MCP tool on all tasks
        if not self.tasks:
            logger.error("No tasks generated by LLM. Exiting.")
            sys.exit(1)
        
        ralphs_result = await self.mcp_client.call_tool("ralph", {
            "messages": self.tasks
        })

        logger.error(f"Ralphs result: {ralphs_result}")

        sys.exit(1)

        while self.tasks:
            task = self.tasks.pop(0)

            messages = [
                {"role": "system", "content": self.agent},
                {"role": "user", "content": self.task}
            ]

            logger.debug(f"Ralph: {self.agent}")

            response = llm_completion(messages, functions=self.tools)

            logger.info(f"Initial LLM response: {response.model_dump()}")

            sys.exit(0)

            while response.function_call:
                function_call = response.function_call

                messages.append(response.model_dump())

                function_name = function_call.name
                arguments = json.loads(function_call.arguments) if isinstance(function_call.arguments, str) else function_call.arguments

                logger.info(f"Calling function: {function_name} with arguments: {arguments}")

                if function_name and arguments:
                    
                    tool_response = await self.mcp_client.call_tool(function_name, arguments)
                    tool_content = tool_response.get("result", {})

                    messages.append({
                        "role": "assistant",
                        "content": tool_content if isinstance(tool_content, str) else json.dumps(tool_content)
                    })

                    response = llm_completion(
                        messages=messages,
                        functions=self.tools,
                    )
                else:
                    logger.error("Invalid function call structure in response.")
            else:
                logger.info(f"Assistant: {response.content if response.content else 'No content returned from LLM.'}")
                messages.append({
                    "role": "assistant",
                    "content": response.content if response.content else "No content returned from LLM."
                    })

def main():
    if len(sys.argv) < 2:
        print("Usage: python ralph.py <message>")
        sys.exit(1)

    overall_task = sys.argv[1]
    agent = "You are Ralph, an autonomous agent that specifies, plans, writes, and fixes software in Python."
    if os.path.exists("AGENT.md"):
        with open("AGENT.md", "r") as f:
            agent = f.read()

    # start the MCP server in a separate process
    mcp_server_process = subprocess.Popen(
        [sys.executable, "src/mcp/server.py"],
        stdin=subprocess.PIPE,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE
    )

    ralph = Ralph(agent, overall_task, server_stdin=mcp_server_process.stdin, server_stdout=mcp_server_process.stdout)
    asyncio.run(ralph.execute())

    mcp_server_process.terminate()
    mcp_server_process.wait()
    sys.exit(0)

if __name__ == "__main__":
    main()